import {FC} from 'react';
import Link from 'next/link';
import {GetStaticPaths, GetStaticProps} from 'next';

// Get basePath for static assets
const basePath = process.env.NODE_ENV === 'production' ? '/personal-website' : '';

// Define all your projects here
const projects: Record<string, {
  title: string;
  description: string;
  videos?: {src: string; caption: string}[];
  images?: {src: string; caption: string}[];
  youtubeIds?: {id: string; caption: string}[];
  sections?: {title: string; content: string | JSX.Element}[];
  links?: {label: string; url: string}[];
}> = {

  // Project 1(Imitation Learning & VLA Development)
  'koch-imitation-learning': {
    title: 'Imitation Learning & VLAs with low-cost robotic arm',
    description: `
      Imagine a robotic arm that can learn to perform tasks 
      just by watching videos and reading instruction!
    `.trim(),
    videos: [
      {src: '/videos/koch_open_draw.mp4', caption: 'Robot opening drawer demo'},
    ],
    images: [
      {src: '/images/portfolio/Franka_open_drawer.png', caption: 'Training process'},
    ],
    youtubeIds: [
      {id: '8RHWoJiWaVc', caption: 'Full demonstration video'},
    ],
    sections: [
      {
        title: 'Overview',
        content: (
          <div className="space-y-4">
            <p>
              This project demonstrates how low-cost robotic arms{' '}
              <strong>
                (<a href="https://github.com/jess-moss/koch-v1-1" target="_blank" rel="noopener noreferrer" className="text-blue-400 hover:underline">Koch</a>)
              </strong>{' '}
              can learn using{' '}
              <strong>Visual Language Action (VLA)</strong>{' '}
              models like{' '}
              <strong>
                <a href="https://huggingface.co/docs/lerobot/smolvla" target="_blank" rel="noopener noreferrer" className="text-blue-400 hover:underline">SmolVLA</a>
              </strong>{' '}
              &{' '}
              <strong>
                <a href="https://huggingface.co/docs/lerobot/groot" target="_blank" rel="noopener noreferrer" className="text-blue-400 hover:underline">GR00T N1.5</a>
              </strong>{' '}
              and{' '}
              <strong>
                imitation learning like{' '}
                <a href="https://huggingface.co/docs/lerobot/act" target="_blank" rel="noopener noreferrer" className="text-blue-400 hover:underline">Action Chunking Transformer (ACT)</a>
              </strong>.
            </p>
            <img 
              src={`${basePath}/images/portfolio/lerobot-pipeline-koch.png`}
              alt="LeRobot Pipeline - How imitation learning models are trained"
              className="w-full rounded-lg shadow-lg mt-4"
            />
            <p className="text-sm text-gray-400 text-center">LeRobot Pipeline: Training imitation learning models by Koch robot</p>
          </div>
        ),
      },
      {
        title: '1. Data Collection',
        content: (
          <div className="space-y-4">
            <p>
              The first step is collecting high-quality demonstration data from human teleoperation.
            </p>
            
            <h3 className="text-lg font-semibold text-white mt-6">Dataset Format</h3>
            <p>
              We use the <strong>LeRobot dataset format</strong>, which stores episodes as Parquet files 
              containing <strong> observation(following robot joint states) </strong>, and <strong> action(leading arm's joint states) </strong> and mp4 for top and front camera observation.
            </p>
            
            <div className="bg-gray-800 rounded-lg p-4 mt-4 font-mono text-sm">
              <p className="text-green-400 mb-2"># Robot Joint States (6-DOF)</p>
              <ul className="text-gray-300 space-y-1 ml-4">
                <li>{'"shoulder_pan.pos"'}</li>
                <li>{'"shoulder_lift.pos"'}</li>
                <li>{'"elbow_flex.pos"'}</li>
                <li>{'"wrist_flex.pos"'}</li>
                <li>{'"wrist_roll.pos"'}</li>
                <li>{'"gripper.pos"'}</li>
              </ul>
              <p className="text-green-400 mb-2 mt-4"># Camera Observations</p>
              <ul className="text-gray-300 space-y-1 ml-4">
                <li>{'"observation.images.front"'}</li>
                <li>{'"observation.images.top"'}</li>
              </ul>
            </div>
            <p className="text-sm text-gray-400 text-center mt-2">LeRobot dataset structure with joint positions and camera observations</p>
            
            <h3 className="text-lg font-semibold text-white mt-6">Control Method</h3>
            <p>
              Human demonstrations are collected via <strong>teleoperation</strong> using a leader-follower 
              setup, where the operator controls a leader arm and the follower arm mimics the movements.
            </p>
              <img 
                src={`${basePath}/images/portfolio/koch-teleop.gif`}
                alt="Leader-follower teleoperation for data collection"
                className="w-full rounded-lg shadow-lg mt-2"
              />
            <p className="text-sm text-gray-400 text-center">Leader-follower teleoperation for data collection</p>
          </div>
        ),
      },
      {
        title: '2. Model Training',
        content: (
          <div className="space-y-6">
            <p>
              After collecting demonstration data, we train imitation learning models to 
              predict robot actions from visual observations. We experimented with three 
              different approaches: <strong>ACT</strong>, <strong>SmolVLA</strong>, and <strong>GR00T N1.5</strong>.
            </p>

            {/* ACT Section */}
            <div className="bg-gray-800/50 rounded-lg p-5 border border-gray-700">
              <h3 className="text-xl font-bold text-blue-400 mb-3">
                ACT (Action Chunking Transformer)
              </h3>
              <p className="mb-3">
                A <strong>pure imitation learning</strong> approach that predicts <strong>action sequences("chunks") rather than single actions </strong> . 
                It uses a transformer encoder-decoder architecture 
                with a CVAE (Conditional Variational Autoencoder) for modeling action distributions.
              </p>
              <div className="my-4">
                <img 
                  src={`${basePath}/images/portfolio/act-paper.png`}
                  alt="ACT Architecture - Action Chunking Transformer from the original paper"
                  className="w-full rounded-lg shadow-lg"
                />
                <p className="text-sm text-gray-400 text-center mt-2">
                  ACT Architecture (Source: <a href="https://arxiv.org/abs/2304.13705" target="_blank" rel="noopener noreferrer" className="text-blue-400 hover:underline">ACT Paper</a>)
                </p>
              </div>
              <div className="grid md:grid-cols-2 gap-4 mt-4">
                <div>
                  <h4 className="text-sm font-semibold text-green-400 mb-2">‚úÖ Strengths</h4>
                  <ul className="text-sm space-y-1 text-gray-300">
                    <li>‚Ä¢ Fast training (no VLM backbone)</li>
                    <li>‚Ä¢ Lightweight (~25M parameters)</li>
                    <li>‚Ä¢ Good for single-task learning</li>
                  </ul>
                </div>
                <div>
                  <h4 className="text-sm font-semibold text-red-400 mb-2">‚ö†Ô∏è Limitations</h4>
                  <ul className="text-sm space-y-1 text-gray-300">
                    <li>‚Ä¢ No language understanding</li>
                    <li>‚Ä¢ Requires task-specific training</li>
                    <li>‚Ä¢ Limited generalization</li>
                  </ul>
                </div>
              </div>
            </div>

            {/* SmolVLA Section */}
            <div className="bg-gray-800/50 rounded-lg p-5 border border-gray-700">
              <h3 className="text-xl font-bold text-orange-400 mb-3">
                SmolVLA (Lightweight Vision-Language-Action)
              </h3>
              <p className="mb-3">
                A <strong>compact VLA model</strong> that combines vision encoders <strong> (SigLIP) </strong> with a small language 
                model backbone <strong> (SmolVLM-2) </strong> . It can understand natural language instructions and map them to robot actions, 
                enabling <strong>language-conditioned control</strong>.
              </p>
              <div className="my-4">
                <img 
                  src={`${basePath}/images/portfolio/smolvla-paper.png`}
                  alt="SmolVLA Architecture - Lightweight Vision-Language-Action Model"
                  className="w-full rounded-lg shadow-lg"
                />
                <p className="text-sm text-gray-400 text-center mt-2">
                  SmolVLA Architecture (Source: <a href="https://arxiv.org/abs/2506.01844" target="_blank" rel="noopener noreferrer" className="text-blue-400 hover:underline">SmolVLA Paper</a>)
                </p>
              </div>
              <div className="grid md:grid-cols-2 gap-4 mt-4">
                <div>
                  <h4 className="text-sm font-semibold text-green-400 mb-2">‚úÖ Strengths</h4>
                  <ul className="text-sm space-y-1 text-gray-300">
                    <li>‚Ä¢ Language-conditioned actions</li>
                    <li>‚Ä¢ Better generalization across tasks</li>
                    <li>‚Ä¢ Smaller than full VLAs (~500M params)</li>
                  </ul>
                </div>
                <div>
                  <h4 className="text-sm font-semibold text-red-400 mb-2">‚ö†Ô∏è Limitations</h4>
                  <ul className="text-sm space-y-1 text-gray-300">
                    <li>‚Ä¢ Slower inference than ACT</li>
                    <li>‚Ä¢ Requires more VRAM (~6GB+)</li>
                    <li>‚Ä¢ Longer training time</li>
                  </ul>
                </div>
              </div>
            </div>

            {/* GR00T N1.5 Section */}
            <div className="bg-gray-800/50 rounded-lg p-5 border border-gray-700">
              <h3 className="text-xl font-bold text-green-400 mb-3">
                GR00T N1.5 (NVIDIA)
              </h3>
              <p className="mb-3">
                NVIDIA's <strong>foundation model for humanoid robots</strong>, designed for generalist 
                robot control. It features a dual-system architecture with a  vision-language 
                reasoning system and a action generation system for real-time control.
              </p>
              <div className="my-4">
                <img 
                  src={`${basePath}/images/portfolio/gr00t-paper.png`}
                  alt="GR00T N1.5 Architecture - NVIDIA's Foundation Model for Humanoid Robots"
                  className="w-full rounded-lg shadow-lg"
                />
                <p className="text-sm text-gray-400 text-center mt-2">
                  GR00T N1.5 Architecture (Source: <a href="https://arxiv.org/abs/2503.14734" target="_blank" rel="noopener noreferrer" className="text-blue-400 hover:underline">GR00T N1.5 Paper</a>)
                </p>
              </div>
              <div className="grid md:grid-cols-2 gap-4 mt-4">
                <div>
                  <h4 className="text-sm font-semibold text-green-400 mb-2">‚úÖ Strengths</h4>
                  <ul className="text-sm space-y-1 text-gray-300">
                    <li>‚Ä¢ Best zero-shot generalization</li>
                    <li>‚Ä¢ Cross-embodiment transfer</li>
                    <li>‚Ä¢ Rich language understanding</li>
                  </ul>
                </div>
                <div>
                  <h4 className="text-sm font-semibold text-red-400 mb-2">‚ö†Ô∏è Limitations</h4>
                  <ul className="text-sm space-y-1 text-gray-300">
                    <li>‚Ä¢ Largest model (~2B+ params)</li>
                    <li>‚Ä¢ Requires significant compute</li>
                    <li>‚Ä¢ Motion jitter in some tasks</li>
                  </ul>
                </div>
              </div>
            </div>

            {/* Comparison Table */}
            <h3 className="text-lg font-semibold text-white mt-8">üìä Model Comparison</h3>
            <div className="overflow-x-auto">
              <table className="w-full text-sm border border-gray-700 rounded-lg overflow-hidden">
                <thead className="bg-gray-800">
                  <tr>
                    <th className="px-4 py-3 text-left">Feature</th>
                    <th className="px-4 py-3 text-center text-blue-400">ACT</th>
                    <th className="px-4 py-3 text-center text-orange-400">SmolVLA</th>
                    <th className="px-4 py-3 text-center text-green-400">GR00T N1.5</th>
                  </tr>
                </thead>
                <tbody className="divide-y divide-gray-700">
                  <tr className="bg-gray-900/50">
                    <td className="px-4 py-2">Language Conditioning</td>
                    <td className="px-4 py-2 text-center">‚ùå</td>
                    <td className="px-4 py-2 text-center">‚úÖ</td>
                    <td className="px-4 py-2 text-center">‚úÖ</td>
                  </tr>
                  <tr className="bg-gray-800/50">
                    <td className="px-4 py-2">Parameters</td>
                    <td className="px-4 py-2 text-center">~25M</td>
                    <td className="px-4 py-2 text-center">~500M</td>
                    <td className="px-4 py-2 text-center">~2-3B</td>
                  </tr>
                  <tr className="bg-gray-900/50">
                    <td className="px-4 py-2">Training Time</td>
                    <td className="px-4 py-2 text-center">~2-4 hrs</td>
                    <td className="px-4 py-2 text-center">~8-12 hrs</td>
                    <td className="px-4 py-2 text-center">~24+ hrs</td>
                  </tr>
                  <tr className="bg-gray-800/50">
                    <td className="px-4 py-2">Min VRAM</td>
                    <td className="px-4 py-2 text-center">4GB</td>
                    <td className="px-4 py-2 text-center">6GB+</td>
                    <td className="px-4 py-2 text-center">24GB+</td>
                  </tr>
                  <tr className="bg-gray-900/50">
                    <td className="px-4 py-2">Best For</td>
                    <td className="px-4 py-2 text-center">Single task</td>
                    <td className="px-4 py-2 text-center">Multi-task</td>
                    <td className="px-4 py-2 text-center">Generalist</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <h3 className="text-lg font-semibold text-white mt-6">Training Configuration</h3>
            <ul className="list-disc pl-6 space-y-1">
              <li><strong>Batch size:</strong> 32 (ACT) / 8 (VLAs)</li>
              <li><strong>Learning rate:</strong> 1e-4 (ACT) / 2e-5 (VLAs)</li>
              <li><strong>Training epochs:</strong> 100 (ACT) / 50 (VLAs)</li>
              <li><strong>GPU:</strong> NVIDIA RTX 4090</li>
            </ul>
          </div>
        ),
      },
      {
        title: '3. Deployment',
        content: (
          <div className="space-y-4">
            <p>
              The trained model is deployed on the robot for real-time inference 
              and autonomous task execution.
            </p>

            {/* Task Setup Section */}
            <h3 className="text-lg font-semibold text-white mt-6">üìã Task Setup</h3>
            <p>
              Before deployment, we define the task setup to ensure consistent evaluation. 
              This includes specifying object positions, grasp configurations, and success criteria.
            </p>
            
            <div className="bg-gray-800 rounded-lg p-4 mt-4">
              <h4 className="text-md font-semibold text-blue-400 mb-3">Dataset Formation</h4>
              <ul className="space-y-2 text-gray-300">
                <li className="flex items-start">
                  <span className="text-green-400 mr-2">‚Ä¢</span>
                  <div>
                    <strong>Object Position:</strong> Objects are placed within a defined workspace region 
                    (e.g., 30cm √ó 30cm area in front of the robot)
                  </div>
                </li>
                <li className="flex items-start">
                  <span className="text-green-400 mr-2">‚Ä¢</span>
                  <div>
                    <strong>Grasp Configuration:</strong> Initial gripper state is fully open; 
                    grasp is triggered when approaching target object
                  </div>
                </li>
                <li className="flex items-start">
                  <span className="text-green-400 mr-2">‚Ä¢</span>
                  <div>
                    <strong>Initial Pose:</strong> Robot starts from a consistent home position 
                    for each episode
                  </div>
                </li>
                <li className="flex items-start">
                  <span className="text-green-400 mr-2">‚Ä¢</span>
                  <div>
                    <strong>Success Criteria:</strong> Task is successful when the object reaches 
                    the target location/state
                  </div>
                </li>
              </ul>
            </div>

            {/* Task Setup Video */}
            <img 
              src={`${basePath}/images/portfolio/three_task_setup.png`}
              alt="Task setup showing object placement and initial configuration"
              className="w-full rounded-lg shadow-lg mt-4"
            />
            <p className="text-sm text-gray-400 text-center">Task setup demonstration showing object placement and initial configuration</p>
            
            <h3 className="text-lg font-semibold text-white mt-6">Real-time Inference</h3>
            <p>
              The model runs at <strong>10Hz</strong>, predicting action chunks that are 
              executed by the robot controller in real-time.
            </p>
            <video
              autoPlay
              loop
              muted
              playsInline
              className="w-full rounded-lg shadow-lg mt-2"
            >
              <source src={`${basePath}/videos/deployment-demo.mp4`} type="video/mp4" />
            </video>
            <p className="text-sm text-gray-400 text-center">Autonomous task execution after deployment</p>
            
            <h3 className="text-lg font-semibold text-white mt-6">System Requirements</h3>
            <ul className="list-disc pl-6 space-y-1">
              <li><strong>Inference hardware:</strong> NVIDIA GPU with 8GB+ VRAM</li>
              <li><strong>Framework:</strong> PyTorch + ROS2 Humble</li>
              <li><strong>Control frequency:</strong> 10Hz action prediction</li>
            </ul>
          </div>
        ),
      },
      {
        title: 'Technical Details',
        content: (
          <div className="space-y-4">
            <p>
              The system uses <span className="font-bold">ROS2</span> for robot control
              and <span className="font-bold">PyTorch</span> for model inference.
            </p>
            <ul className="list-disc pl-6 space-y-2">
              <li><strong>Framework:</strong> ROS2 Humble</li>
              <li><strong>Model:</strong> Action Chunking Transformer</li>
              <li><strong>Inference Speed:</strong> 10Hz real-time</li>
            </ul>
          </div>
        ),
      },
    ],
    links: [
      {label: 'GitHub', url: 'https://github.com/yourusername/franka-vla'},
      {label: 'Paper', url: 'https://arxiv.org/abs/xxxx.xxxxx'},
    ],
  },

  // Project 2(Imitation Learning & VLA Development)
  'franka-imitation-learning': {
    title: 'Imitation Learning & VLAs with low-cost robotic arm',
    description: `
      Imagine a robotic arm that can learn to perform tasks 
      just by watching videos and reading instruction!
    `.trim(),
    videos: [
      {src: '/videos/franka_open_draw.mp4', caption: 'Robot opening drawer demo'},
    ],
    images: [
      {src: '/images/portfolio/franka-1.png', caption: 'System setup'},
      {src: '/images/portfolio/franka-2.png', caption: 'Training process'},
    ],
    youtubeIds: [
      {id: '8RHWoJiWaVc', caption: 'Full demonstration video'},
    ],
    sections: [
      {
        title: 'Overview',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models like <strong>SmolVLA & GR00T N1.5 </strong>and{' '}
            <strong>imitation learning like Action Chunking Transformer(ACT) </strong>.
          </p>
        ),
      },
      {
        title: 'Project Details',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models and{' '}
            <strong>imitation learning</strong>.
          </p>
        ),
      },
      {
        title: 'Technical Details',
        content: (
          <div className="space-y-4">
            <p>
              The system uses <span className="font-bold">ROS2</span> for robot control
              and <span className="font-bold">PyTorch</span> for model inference.
            </p>
            <ul className="list-disc pl-6 space-y-2">
              <li><strong>Framework:</strong> ROS2 Humble</li>
              <li><strong>Model:</strong> Action Chunking Transformer</li>
              <li><strong>Inference Speed:</strong> 10Hz real-time</li>
            </ul>
          </div>
        ),
      },
    ],
    links: [
      {label: 'GitHub', url: 'https://github.com/yourusername/franka-vla'},
      {label: 'Paper', url: 'https://arxiv.org/abs/xxxx.xxxxx'},
    ],
  },

  // Project 3(Imitation Learning & VLA Development)
  'koch-vla': {
    title: 'Imitation Learning & VLAs with low-cost robotic arm',
    description: `
      Imagine a robotic arm that can learn to perform tasks 
      just by watching videos and reading instruction!
    `.trim(),
    videos: [
      {src: '/videos/koch_open_drawer.mp4', caption: 'Robot opening drawer demo'},
    ],
    images: [
      {src: '/images/portfolio/koch-1.png', caption: 'System setup'},
      {src: '/images/portfolio/koch-2.png', caption: 'Training process'},
    ],
    youtubeIds: [
      {id: '8RHWoJiWaVc', caption: 'Full demonstration video'},
    ],
    sections: [
      {
        title: 'Overview',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models like <strong>SmolVLA & GR00T N1.5 </strong>and{' '}
            <strong>imitation learning like Action Chunking Transformer(ACT) </strong>.
          </p>
        ),
      },
      {
        title: 'Project Details',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models and{' '}
            <strong>imitation learning</strong>.
          </p>
        ),
      },
      {
        title: 'Technical Details',
        content: (
          <div className="space-y-4">
            <p>
              The system uses <span className="font-bold">ROS2</span> for robot control
              and <span className="font-bold">PyTorch</span> for model inference.
            </p>
            <ul className="list-disc pl-6 space-y-2">
              <li><strong>Framework:</strong> ROS2 Humble</li>
              <li><strong>Model:</strong> Action Chunking Transformer</li>
              <li><strong>Inference Speed:</strong> 10Hz real-time</li>
            </ul>
          </div>
        ),
      },
    ],
    links: [
      {label: 'GitHub', url: 'https://github.com/yourusername/franka-vla'},
      {label: 'Paper', url: 'https://arxiv.org/abs/xxxx.xxxxx'},
    ],
  },

  // Project 4(Imitation Learning & VLA Development)
  'attention-heat-map': {
    title: 'Attention Heat Map Visualization for VLA Models',
    description: `
      Visualizing attention maps for VLA models to understand how the model focuses on different parts of the input.
    `.trim(),
    videos: [
      {src: '/videos/franka_open_draw.mp4', caption: 'Robot opening drawer demo'},
    ],
    images: [
      {src: '/images/portfolio/franka-1.png', caption: 'System setup'},
      {src: '/images/portfolio/franka-2.png', caption: 'Training process'},
    ],
    youtubeIds: [
      {id: '8RHWoJiWaVc', caption: 'Full demonstration video'},
    ],
    sections: [
      {
        title: 'Overview',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models like <strong>SmolVLA & GR00T N1.5 </strong>and{' '}
            <strong>imitation learning like Action Chunking Transformer(ACT) </strong>.
          </p>
        ),
      },
      {
        title: 'Project Details',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models and{' '}
            <strong>imitation learning</strong>.
          </p>
        ),
      },
      {
        title: 'Technical Details',
        content: (
          <div className="space-y-4">
            <p>
              The system uses <span className="font-bold">ROS2</span> for robot control
              and <span className="font-bold">PyTorch</span> for model inference.
            </p>
            <ul className="list-disc pl-6 space-y-2">
              <li><strong>Framework:</strong> ROS2 Humble</li>
              <li><strong>Model:</strong> Action Chunking Transformer</li>
              <li><strong>Inference Speed:</strong> 10Hz real-time</li>
            </ul>
          </div>
        ),
      },
    ],
    links: [
      {label: 'GitHub', url: 'https://github.com/yourusername/franka-vla'},
      {label: 'Paper', url: 'https://arxiv.org/abs/xxxx.xxxxx'},
    ],
  },

  // Autonomous Navigation System Development

  // Project 5(Autonomous Navigation System Development for turtlebot)
  'turtlebot-nav': {
    title: 'Multi-map Switching System for Robot Navigation',
    description: `
      Developing a multi-map switching system for autonomous navigation of a TurtleBot in dynamic environments.
    `.trim(),
    videos: [
      {src: '/videos/turtlebot_multi_map.mp4', caption: 'Multi-map switching demo'},
    ],
    images: [
      {src: '/images/portfolio/turtlebot-1.png', caption: 'System setup'},
      {src: '/images/portfolio/turtlebot-2.png', caption: 'Training process'},
    ],
    youtubeIds: [
      {id: '8RHWoJiWaVc', caption: 'Full demonstration video'},
    ],
    sections: [
      {
        title: 'Overview',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models like <strong>SmolVLA & GR00T N1.5 </strong>and{' '}
            <strong>imitation learning like Action Chunking Transformer(ACT) </strong>.
          </p>
        ),
      },
      {
        title: 'Project Details',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models and{' '}
            <strong>imitation learning</strong>.
          </p>
        ),
      },
      {
        title: 'Technical Details',
        content: (
          <div className="space-y-4">
            <p>
              The system uses <span className="font-bold">ROS2</span> for robot control
              and <span className="font-bold">PyTorch</span> for model inference.
            </p>
            <ul className="list-disc pl-6 space-y-2">
              <li><strong>Framework:</strong> ROS2 Humble</li>
              <li><strong>Model:</strong> Action Chunking Transformer</li>
              <li><strong>Inference Speed:</strong> 10Hz real-time</li>
            </ul>
          </div>
        ),
      },
    ],
    links: [
      {label: 'GitHub', url: 'https://github.com/yourusername/turtlebot-multi-map'},
      {label: 'Paper', url: 'https://arxiv.org/abs/xxxx.xxxxx'},
    ],
  },
  
  // Project 6(Autonomous Navigation System Development for spot)
  'spot-nav': {
    title: 'Multi-map Switching System for Spot Robot Navigation',
    description: `
      Developing a multi-map switching system for autonomous navigation of a Spot robot in dynamic environments.
    `.trim(),
    videos: [
      {src: '/videos/spot_multi_map.mp4', caption: 'Multi-map switching demo'},
    ],
    images: [
      {src: '/images/portfolio/spot-1.png', caption: 'System setup'},
      {src: '/images/portfolio/spot-2.png', caption: 'Training process'},
    ],
    youtubeIds: [
      {id: '8RHWoJiWaVc', caption: 'Full demonstration video'},
    ],
    sections: [
      {
        title: 'Overview',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models like <strong>SmolVLA & GR00T N1.5 </strong>and{' '}
            <strong>imitation learning like Action Chunking Transformer(ACT) </strong>.
          </p>
        ),
      },
      {
        title: 'Project Details',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models and{' '}
            <strong>imitation learning</strong>.
          </p>
        ),
      },
      {
        title: 'Technical Details',
        content: (
          <div className="space-y-4">
            <p>
              The system uses <span className="font-bold">ROS2</span> for robot control
              and <span className="font-bold">PyTorch</span> for model inference.
            </p>
            <ul className="list-disc pl-6 space-y-2">
              <li><strong>Framework:</strong> ROS2 Humble</li>
              <li><strong>Model:</strong> Action Chunking Transformer</li>
              <li><strong>Inference Speed:</strong> 10Hz real-time</li>
            </ul>
          </div>
        ),
      },
    ],
    links: [
      {label: 'GitHub', url: 'https://github.com/yourusername/turtlebot-multi-map'},
      {label: 'Paper', url: 'https://arxiv.org/abs/xxxx.xxxxx'},
    ],
  },

  // Project 7(Autonomous Visual Navigation System Development for triceratops)
  'triceratops-nav': {
    title: 'Visual Navigation System for Triceratops Robot',
    description: `
      Developing a visual navigation system for autonomous navigation of a Triceratops robot in indoor environments.
    `.trim(),
    videos: [
      {src: '/videos/triceratops_visual_nav.mp4', caption: 'Visual navigation demo'},
    ],
    images: [
      {src: '/images/portfolio/triceratops-1.png', caption: 'System setup'},
      {src: '/images/portfolio/triceratops-2.png', caption: 'Training process'},
    ],
    youtubeIds: [
      {id: '8RHWoJiWaVc', caption: 'Full demonstration video'},
    ],
    sections: [
      {
        title: 'Overview',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models like <strong>SmolVLA & GR00T N1.5 </strong>and{' '}
            <strong>imitation learning like Action Chunking Transformer(ACT) </strong>.
          </p>
        ),
      },
      {
        title: 'Project Details',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models and{' '}
            <strong>imitation learning</strong>.
          </p>
        ),
      },
      {
        title: 'Technical Details',
        content: (
          <div className="space-y-4">
            <p>
              The system uses <span className="font-bold">ROS2</span> for robot control
              and <span className="font-bold">PyTorch</span> for model inference.
            </p>
            <ul className="list-disc pl-6 space-y-2">
              <li><strong>Framework:</strong> ROS2 Humble</li>
              <li><strong>Model:</strong> Action Chunking Transformer</li>
              <li><strong>Inference Speed:</strong> 10Hz real-time</li>
            </ul>
          </div>
        ),
      },
    ],
    links: [
      {label: 'GitHub', url: 'https://github.com/yourusername/turtlebot-multi-map'},
      {label: 'Paper', url: 'https://arxiv.org/abs/xxxx.xxxxx'},
    ],
  },

  // Project 8(Autonomous Visual Navigation (local planner) for triceratops)
  'triceratops-local-planner': {
    title: 'Local Planner for Triceratops Visual Navigation System',
    description: `
      Developing a local planner for autonomous navigation of a Triceratops robot in indoor environments.
    `.trim(),
    videos: [
      {src: '/videos/triceratops_visual_nav.mp4', caption: 'Visual navigation demo'},
    ],
    images: [
      {src: '/images/portfolio/triceratops-1.png', caption: 'System setup'},
      {src: '/images/portfolio/triceratops-2.png', caption: 'Training process'},
    ],
    youtubeIds: [
      {id: '8RHWoJiWaVc', caption: 'Full demonstration video'},
    ],
    sections: [
      {
        title: 'Overview',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models like <strong>SmolVLA & GR00T N1.5 </strong>and{' '}
            <strong>imitation learning like Action Chunking Transformer(ACT) </strong>.
          </p>
        ),
      },
      {
        title: 'Project Details',
        content: (
          <p>
            This project demonstrates how robotic arms can learn using{' '}
            <strong>Visual Language Action (VLA)</strong> models and{' '}
            <strong>imitation learning</strong>.
          </p>
        ),
      },
      {
        title: 'Technical Details',
        content: (
          <div className="space-y-4">
            <p>
              The system uses <span className="font-bold">ROS2</span> for robot control
              and <span className="font-bold">PyTorch</span> for model inference.
            </p>
            <ul className="list-disc pl-6 space-y-2">
              <li><strong>Framework:</strong> ROS2 Humble</li>
              <li><strong>Model:</strong> Action Chunking Transformer</li>
              <li><strong>Inference Speed:</strong> 10Hz real-time</li>
            </ul>
          </div>
        ),
      },
    ],
    links: [
      {label: 'GitHub', url: 'https://github.com/yourusername/turtlebot-multi-map'},
      {label: 'Paper', url: 'https://arxiv.org/abs/xxxx.xxxxx'},
    ],
  },
};

// Required for static export - generates all project pages at build time
export const getStaticPaths: GetStaticPaths = async () => {
  const paths = Object.keys(projects).map((slug) => ({
    params: {slug},
  }));

  return {
    paths,
    fallback: false,
  };
};

export const getStaticProps: GetStaticProps = async ({params}) => {
  const slug = params?.slug as string;
  
  // Check if project exists
  if (!projects[slug]) {
    return {
      notFound: true,
    };
  }

  return {
    props: {
      slug,
    },
  };
};

const ProjectPage: FC<{slug: string}> = ({slug}) => {
  const project = projects[slug];

  // Handle project not found
  if (!project) {
    return (
      <div className="min-h-screen bg-neutral-900 text-white flex flex-col items-center justify-center">
        <h1 className="text-4xl font-bold mb-4">Project Not Found</h1>
        <Link href="/#portfolio" className="text-blue-400 hover:underline">
          ‚Üê Back to Portfolio
        </Link>
      </div>
    );
  }

  return (
    <div className="min-h-screen bg-neutral-900 text-white">
      <div className="max-w-4xl mx-auto px-4 py-12">
        {/* Back Link */}
        <Link href="/#portfolio" className="text-blue-400 hover:underline mb-8 inline-block">
          ‚Üê Back to Portfolio
        </Link>

        {/* Title */}
        <section className="mb-12">
          <h1 className="text-4xl font-bold mb-4">{project.title}</h1>
          <p className="text-xl text-gray-300">{project.description}</p>
        </section>

        {/* Text Sections */}
        {project.sections?.map((section, index) => (
          <section key={index} className="mb-12">
            <h2 className="text-2xl font-bold mb-4 border-b border-gray-700 pb-2">
              {section.title}
            </h2>
            <p className="text-gray-300 leading-relaxed">{section.content}</p>
          </section>
        ))}

        {/* Videos */}
        {project.videos && project.videos.length > 0 && (
          <section className="mb-12">
            <h2 className="text-2xl font-bold mb-4 border-b border-gray-700 pb-2">Videos</h2>
            <div className="space-y-6">
              {project.videos.map((video, index) => (
                <div key={index}>
                  <video
                    autoPlay
                    loop
                    muted
                    playsInline
                    controls
                    className="w-full rounded-lg shadow-lg"
                  >
                    <source src={`${basePath}${video.src}`} type="video/mp4" />
                  </video>
                  <p className="text-sm text-gray-400 mt-2">{video.caption}</p>
                </div>
              ))}
            </div>
          </section>
        )}

        {/* Images */}
        {project.images && project.images.length > 0 && (
          <section className="mb-12">
            <h2 className="text-2xl font-bold mb-4 border-b border-gray-700 pb-2">Images</h2>
            <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
              {project.images.map((image, index) => (
                <div key={index}>
                  <img
                    src={`${basePath}${image.src}`}
                    alt={image.caption}
                    className="w-full h-64 object-cover rounded-lg"
                  />
                  <p className="text-sm text-gray-400 mt-2">{image.caption}</p>
                </div>
              ))}
            </div>
          </section>
        )}

        {/* YouTube Videos */}
        {project.youtubeIds && project.youtubeIds.length > 0 && (
          <section className="mb-12">
            <h2 className="text-2xl font-bold mb-4 border-b border-gray-700 pb-2">
              YouTube Videos
            </h2>
            <div className="space-y-6">
              {project.youtubeIds.map((yt, index) => (
                <div key={index}>
                  <div className="relative w-full" style={{paddingBottom: '56.25%'}}>
                    <iframe
                      className="absolute top-0 left-0 w-full h-full rounded-lg"
                      src={`https://www.youtube.com/embed/${yt.id}`}
                      title={yt.caption}
                      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                      allowFullScreen
                    />
                  </div>
                  <p className="text-sm text-gray-400 mt-2">{yt.caption}</p>
                </div>
              ))}
            </div>
          </section>
        )}

        {/* Links */}
        {project.links && project.links.length > 0 && (
          <section className="mb-12">
            <h2 className="text-2xl font-bold mb-4 border-b border-gray-700 pb-2">Links</h2>
            <div className="flex flex-wrap gap-4">
              {project.links.map((link, index) => (
                <a
                  key={index}
                  href={link.url}
                  target="_blank"
                  rel="noopener noreferrer"
                  className="px-4 py-2 bg-gray-700 hover:bg-gray-600 rounded-lg transition"
                >
                  {link.label}
                </a>
              ))}
            </div>
          </section>
        )}

      </div>
    </div>
  );
};

export default ProjectPage;